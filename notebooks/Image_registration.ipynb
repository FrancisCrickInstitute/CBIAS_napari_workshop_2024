{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de0771b-9855-4c6f-93c1-530232dc5b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import napari\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io\n",
    "from skimage import data\n",
    "from skimage.feature import SIFT, match_descriptors\n",
    "from skimage.transform import AffineTransform, warp, estimate_transform\n",
    "from skimage.measure import ransac\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de080586-0f63-4e22-a52a-3527a89ad671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the napari viewer\n",
    "viewer = napari.Viewer()\n",
    "napari.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179536a8-e3d3-4964-925c-c4e8ac491d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the demo data\n",
    "image_original = data.skin()\n",
    "image_warped = io.imread('skin_warped.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769c0e68-aa09-41e2-bad0-3a35d908d4ae",
   "metadata": {},
   "source": [
    "## Explore affine transforms\n",
    "\n",
    "Affine transforms can be applied directly to napari layers. \n",
    "\n",
    "Run the cells below to see how an image layer can be transformed using different affine transform matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46e6c935-c6f2-49da-ae47-80b0b8656427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the example image to the viewer\n",
    "image_layer = viewer.add_image(image_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65128ae8-9460-46d8-ba67-9895c28c73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an itentity matrix\n",
    "identity_transform = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "\n",
    "# Apply the transform to the image layer\n",
    "image_layer.affine = identity_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64afc319-7ad0-43fb-b16e-0dba25f442b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rotation matrix\n",
    "rotation_transform = np.array([\n",
    "    [ np.cos(np.radians(30)), np.sin(np.radians(30)), 0],\n",
    "    [-np.sin(np.radians(30)), np.cos(np.radians(30)), 0],\n",
    "    [0,                       0,                      1],\n",
    "])\n",
    "\n",
    "# Apply the transform to the image layer\n",
    "image_layer.affine = rotation_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f1722e-f015-46be-bc1a-3c52b6107cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a scaling matrix\n",
    "scaling_transform = np.array([\n",
    "    [2, 0, 0],\n",
    "    [0, 3, 0],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "\n",
    "# Apply the transform to the image layer\n",
    "image_layer.affine = scaling_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd940490-cba7-4485-929e-85f74ca080b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rotation matrix\n",
    "translation_transform = np.array([\n",
    "    [1, 0, 200],\n",
    "    [0, 1, 400],\n",
    "    [0, 0, 1],\n",
    "])\n",
    "\n",
    "# Apply the transform to the image layer\n",
    "image_layer.affine = translation_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1d20cfa-9132-4fc7-9d75-d0227af818b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the rotation and scaling transforms\n",
    "combined_transform = rotation_transform @ scaling_transform\n",
    "\n",
    "# Apply the transform to the image layer\n",
    "image_layer.affine = combined_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40473cfb-79c8-4838-b8a2-75b11e9e0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an affine matrix\n",
    "affine_transform = np.array([\n",
    "    [1.09,  0.646, 79.6],\n",
    "    [0.103, 0.966, 639],\n",
    "    [0,     0,     1],\n",
    "])\n",
    "\n",
    "# Apply the transform to the image layer\n",
    "image_layer.affine = affine_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c13ed3-d2a2-45e9-8714-7d7e819c9d24",
   "metadata": {},
   "source": [
    "## Manual image registration\n",
    "\n",
    "An affine matrix to register two images can be found given two sets of matching points.\n",
    "\n",
    "Matching points in both images can be selected with a points layer in napari.\n",
    "\n",
    "Once the affine matrix is calculated, it can be applied to the moving image layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d863697-6fdb-42b3-afd2-bc34967f47a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the example image to the viewer\n",
    "image_layer = viewer.add_image(image_original)\n",
    "\n",
    "# Add the transformed image to the viewer\n",
    "image_warped_layer = viewer.add_image(image_warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75221179-8000-4d29-b326-cb1a660ef2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add points layers to the viewer\n",
    "moving_points = viewer.add_points(name='moving points', face_color='blue')\n",
    "fixed_points = viewer.add_points(name='fixed points', face_color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76220991-cd78-4365-ad95-8cba8d0f9c5c",
   "metadata": {},
   "source": [
    "Before running the next cell, select matching points in both images using the `moving_points` and `fixed_points` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baf9440f-4494-4bf6-a0a9-17f4ab4c8e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine transform matrix:\n",
      "[[   1.74907169   -0.97545893  233.37093391]\n",
      " [   1.5132359     2.57008217 -624.96186827]\n",
      " [   0.            0.            1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the coordinates from the points layers\n",
    "moving_coords = moving_points.data\n",
    "fixed_coords = fixed_points.data\n",
    "\n",
    "# Calculate the affine transform to convert from the moving layer to the fixed layer\n",
    "affine_transform = estimate_transform('affine', moving_coords, fixed_coords)\n",
    "\n",
    "print('Affine transform matrix:')\n",
    "print(affine_transform.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77810d8f-1b7b-4f44-857c-457d6df1db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transform to the moving image and points layers\n",
    "image_warped_layer.affine = affine_transform.params\n",
    "moving_points.affine = affine_transform.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d5682-7f64-4522-86f5-3939677546ad",
   "metadata": {},
   "source": [
    "## Image registration with affinder\n",
    "[affinder](https://github.com/jni/affinder) is a plugin that uses a similar method to find the affine matrix to register two images.\n",
    "\n",
    "To use the plugin, open napari and go to Plugins->Install/Uninstall Plugins and search for 'affinder'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cc0ad3c-12d4-48e7-b53e-5c0cbb07228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the example image to the viewer\n",
    "image_layer = viewer.add_image(image_original)\n",
    "\n",
    "# Add the transformed image to the viewer\n",
    "image_warped_layer = viewer.add_image(image_warped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20225fe2-2e6a-439b-93ec-72d3e73e3661",
   "metadata": {},
   "source": [
    "## Image registration with SIFT\n",
    "[Scale-Invariant Feature Transform (SIFT)](https://doi.org/10.1023/B:VISI.0000029664.99615.94) is a method to register two images using extracted keypoints and features. \n",
    "\n",
    "The approach has three steps:\n",
    "1. Keypoints are extracted from distinctive points in both images (at corners, edges etc). At each keypoint, feature descriptors are also extracted.\n",
    "2. Using the feature descriptors, matched points are found in both images.\n",
    "3. The matched points are used to find the transform matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32abce29-b080-442f-b367-70c743db4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the example image to the viewer\n",
    "image_layer = viewer.add_image(image_original)\n",
    "\n",
    "# Add the transformed image to the viewer\n",
    "image_warped_layer = viewer.add_image(image_warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db116757-07b9-47ff-8f3a-f9e05934175b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Points layer 'fixed_keypoints' at 0x135d50b90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract keypoints and features from the fixed image\n",
    "sift_fixed = SIFT()\n",
    "sift_fixed.detect_and_extract(rgb2gray(image_original))\n",
    "fixed_keypoints = sift_fixed.keypoints\n",
    "fixed_descriptors = sift_fixed.descriptors\n",
    "\n",
    "# Add the fixed keypoints to the viewer\n",
    "viewer.add_points(fixed_keypoints, face_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696915d8-4694-4c5c-ba61-7d8743cda2f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Points layer 'moving_keypoints' at 0x136aa5490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract keypoints and features from the moving image\n",
    "sift_moving = SIFT()\n",
    "sift_moving.detect_and_extract(rgb2gray(image_warped))\n",
    "moving_keypoints = sift_moving.keypoints\n",
    "moving_descriptors = sift_moving.descriptors\n",
    "\n",
    "# Add the moving keypoints to the viewer\n",
    "viewer.add_points(moving_keypoints, face_color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62da3515-323c-43f6-84b1-e3beaccdb7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find matches between moving and fixed points\n",
    "matches = match_descriptors(moving_descriptors, fixed_descriptors, cross_check=True)\n",
    "moving_keypoints_matched = moving_keypoints[matches[:, 0]]\n",
    "fixed_keypoints_matched = fixed_keypoints[matches[:, 1]]\n",
    "\n",
    "# Add the matched points to the viewer\n",
    "moving_points = viewer.add_points(moving_keypoints_matched, face_color='blue')\n",
    "fixed_points = viewer.add_points(fixed_keypoints_matched, face_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ea1546-c71d-436d-bfb9-8e52c6c794f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Affine transform matrix:\n",
      "[[   1.72942948   -0.99827763  239.80786452]\n",
      " [   1.49543738    2.60136086 -623.55216864]\n",
      " [   0.            0.            1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Use ransac to estimate the affine transform\n",
    "affine_transform, inliers = ransac(\n",
    "    (moving_keypoints_matched, fixed_keypoints_matched),\n",
    "    AffineTransform,\n",
    "    min_samples=4,\n",
    "    residual_threshold=2,\n",
    ")\n",
    "\n",
    "moving_points_inliers = viewer.add_points(\n",
    "    moving_keypoints_matched[inliers], \n",
    "    name='moving_points_inliers', \n",
    "    face_color='blue')\n",
    "\n",
    "fixed_points_inliers = viewer.add_points(\n",
    "    fixed_keypoints_matched[inliers], \n",
    "    name='fixed_points_inliers', \n",
    "    face_color='red')\n",
    "\n",
    "# Print the transformation matrix\n",
    "print(\"Affine transform matrix:\")\n",
    "print(affine_transform.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86467cd5-bf22-4e6c-8dc3-7158e48ec4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transform to the transformed image layer\n",
    "image_warped_layer.affine = affine_transform.params\n",
    "moving_points_inliers.affine = affine_transform.params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9668acd5-cec0-49a2-9747-0710ca821de4",
   "metadata": {},
   "source": [
    "So far, the affine matrices have only been applied to the napari layer which doesn't alter the data.\n",
    "\n",
    "We can use the `warp` function to apply the transform directly to the image.\n",
    "\n",
    "Scikit-image expects the transform to be given in (x, y) format whereas we have calculated it in (y, x) format using the numpy standard. It also expects the inverse transform that maps from the original image to the transformed image.\n",
    "\n",
    "We first need to permute the transform and then calculate the inverse to obtain the correct format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a6cbf45-41dc-4a35-bb3c-768b76f1ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permute the transform matrix\n",
    "affine_transform_permuted = affine_transform.params[np.ix_([1, 0, 2], [1, 0, 2])]\n",
    "affine_transform_inverse = np.linalg.inv(affine_transform_permuted)\n",
    "\n",
    "# Compute the transformed image\n",
    "image_aligned = warp(\n",
    "    image_warped, \n",
    "    affine_transform_inverse,  # inverse transform in [x, y, z] order\n",
    "    output_shape=image_original.shape,  # use the original image shape,\n",
    "    preserve_range=True  # keep values between 0-255\n",
    ")\n",
    "\n",
    "# Convert to uint8 before saving\n",
    "image_aligned = image_aligned.astype(np.uint8)\n",
    "\n",
    "# Save the transformed image\n",
    "io.imsave('skin_aligned.png', image_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244dff5e-baea-4f26-b8a2-cf999006030a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
